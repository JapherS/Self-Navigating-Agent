# Self-Navigating Agent
Team members: Japher Su, Furong Tian, Weiqiang Huang
## Introduction
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The challenge chosen in this project is self-navigating agent. Specifically speaking, the problem we wanted to solve is finding a way to have a driving agent self-navigate through a racing track and reach the finish line without crashing on the side of the track or without going off the road. This challenge topic was chosen because self-driving cars in reality could be of huge benefit to long-distance drivers and environments. Self-driving cars may solve traffic jams, and they contribute to stress-free parking and accessibility to transportation. Automation can help reduce the number of crashes on our roads. Government data identifies driver behavior or human error as a factor in 94% of car accidents, and self-driving vehicles help reduce driver error (Benefits of Self-Driving Vehicles, 2018). Highly autonomous vehicles (HAVs) have the potential to reduce fuel consumption and carbon emissions.  Reduced traffic jams save fuel and reduce greenhouse gases from needless idling (Benefits of Self-Driving Vehicles, 2018).<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additionally, wide-scale deployment of HAVs could allow drivers to recapture time. They could offer the convenience of dropping off passengers at their destination, whether it’s an airport or shopping mall, while the vehicle parks itself (Benefits of Self-Driving Vehicles, 2018). In a fully automated vehicle, all vehicle occupants could safely pursue productive or entertaining activities inside the vehicle, such as responding to emails or watching movies. A lot of big companies (such as Google, Lyft, Uber, and Synopsys) are spending a huge amount of time and energy into developing autonomous cars. These reasons are why it is a hot topic in the technology world nowadays. And these reasons are also why self-driving cars was chosen as the topic in the current project.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To solve the proposed problem, Q-learning, which is a type of reinforcement learning, was selected as our strategy. Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Reinforcement learning involves an agent, a set of states S, and a set A of actions per state. By performing an action a in A, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score). The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state (Wikipedia contributors, 2021).<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-learning is a model-free reinforcement learning algorithm that learns the value of an action in a particular state. It does not require a model of the environment (hence "model-free"), and it can handle situations with stochastic transitions and rewards without requiring adaptations. Q-learning has been used to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward (Matiisen, 2015). That indicates that Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state (Russell, 2010). The letter "Q" refers to the quality value that the function of algorithm computes - the expected rewards for an action taken in a given state (Matiisen, 2015). This Q value is the sum of a current value weighted by its learning rate, the reward to obtain if action a is taken when in state s (weighted by the learning rate), and the maximum reward that can be obtained from the next state s’ (Wikipedia contributors, 2021).<br/>
***

