![](https://github.com/JapherS/Self-Navigating-Agent/blob/master/learning_plots/Screenshot.png)

# Self-Navigating Agent
Team members: Japher S., Furong T., Weiqiang H.
***

## Introduction
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The challenge chosen in this project is self-navigating agent. Specifically speaking, the problem we wanted to solve is finding a way to have a driving agent self-navigate through a racing track and reach the finish line without crashing on the side of the track or without going off the road. This challenge topic was chosen because self-driving cars in reality could be of huge benefit to long-distance drivers and environments. Self-driving cars may solve traffic jams, and they contribute to stress-free parking and accessibility to transportation. Automation can help reduce the number of crashes on our roads. Government data identifies driver behavior or human error as a factor in 94% of car accidents, and self-driving vehicles help reduce driver error (Benefits of Self-Driving Vehicles, 2018). Highly autonomous vehicles (HAVs) have the potential to reduce fuel consumption and carbon emissions.  Reduced traffic jams save fuel and reduce greenhouse gases from needless idling (Benefits of Self-Driving Vehicles, 2018).<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Additionally, wide-scale deployment of HAVs could allow drivers to recapture time. They could offer the convenience of dropping off passengers at their destination, whether it’s an airport or shopping mall, while the vehicle parks itself (Benefits of Self-Driving Vehicles, 2018). In a fully automated vehicle, all vehicle occupants could safely pursue productive or entertaining activities inside the vehicle, such as responding to emails or watching movies. A lot of big companies (such as Google, Lyft, Uber, and Synopsys) are spending a huge amount of time and energy into developing autonomous cars. These reasons are why it is a hot topic in the technology world nowadays. And these reasons are also why self-driving cars was chosen as the topic in the current project.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To solve the proposed problem, Q-learning, which is a type of reinforcement learning, was selected as our strategy. Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning. Reinforcement learning involves an agent, a set of states S, and a set A of actions per state. By performing an action a in A, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score). The goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state (Wikipedia contributors, 2021).<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-learning is a model-free reinforcement learning algorithm that learns the value of an action in a particular state. It does not require a model of the environment (hence "model-free"), and it can handle situations with stochastic transitions and rewards without requiring adaptations. Q-learning has been used to solve the credit assignment problem – it propagates rewards back in time, until it reaches the crucial decision point which was the actual cause for the obtained reward (Matiisen, 2015). That indicates that Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state (Russell, 2010). The letter "Q" refers to the quality value that the function of algorithm computes - the expected rewards for an action taken in a given state (Matiisen, 2015). This Q value is the sum of a current value weighted by its learning rate, the reward to obtain if action a is taken when in state s (weighted by the learning rate), and the maximum reward that can be obtained from the next state s’ (Wikipedia contributors, 2021).<br/>
***

## Related Work
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ponteves and Eremenko’s educational project (2021) provides background information about AI and gives three example projects as a demonstration. One of their projects has a focus on self-navigating car. Their project was set out to train an agent to navigate around obstacles to reach a specified goal position from a starting position. The two positions could be set at any point on the map and the obstacles could also be freely drawn. Their self-driving car could learn to navigate through trial and error and reach the goal with enough training and good learning parameters. In the project, they utilized deep Q-learning which is a combination of Q-learning and neural network. In deep Q-learning, a neural network is used to approximate the Q-value function. A state is usually given as the input and a Q-value of all possible actions is generated as the output.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the current project, we focused on a more specific scenario of self-navigation, which is a self-driving car navigating on a fixed track. Furthermore, Q-learning was selected as the learning strategy for the agent with the intention of building a solid foundation of reinforcement learning.<br/> 
***

## Implementation and Environment
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A racing car environment that is implemented by using Pygame (Pete, 2011) and OpenAI Gym (Brockman et al., 2016), which are popular Python libraries for coding video games and setting up the environment, were used as framework in the current project. The project was written in Python and structured into modularized file:
  - ***Game.py***: This class contains the main game and the logic of training the reinforcement learning agent. 
  - ***QLearning.py*** and ***SARSA.py***: These two classes represent the functions/components that make up a Q-learning agent and a SARSA agent (e.g., update the q-table and choose the next action based on the values from the q-table).	
  - The folder ***gym_race.env*** contains all the environment-related files and the following information/functions:
    - Many attributes that fully define a car’s state, such as angle, position, speed, and the status of whether the car has crashed on the side of the track.
    - Usage of PyGame library to create the game UI.
    - Utility functions such as between-points calculation and reward evaluation.
***

## Approach 
### ***Q-Learning*** 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-learning algorithm was used to solve the self-navigating car problem with a pre-defined map. In the Q-learning algorithm, there are two important factors in that influence the learning efficiency and the outcome of an agent: alpha and gamma. Both values range from 0 to 1. Alpha is the learning rate of an agent. A greater alpha value means the agent will update his Q-values to a greater extent as he explores. Gamma is the discount factor of learning. The greater the discount factor, the more the agent takes the previously acquired knowledge into account. After trying and comparing different possible values while observing the overall performance of the agent, the final alpha and gamma values chosen were 0.15 and 0.99.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In Reinforcement Learning, action decision is made up of two parts: one is exploitation (when the agent keep adopting the same action selection policy), and the other one is exploration (when the agent tries something new or not adhering to the existing policy) (Salloum, 2019). Exploration indicates that the agent searches over the entire sample space, while exploitation indicates that the agent are exploites the promising areas found from previous exploration. Always exploring things that are sub-optimal can be a waste of time and resources. For this reason, it is important to use exploration methods that minimize regrets, so that the learning phase becomes faster and more efficient (Salloum, 2019).<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;An epsilon parameter is used to mark the trade-off between exploration and exploitation. In the beginning of the learning, it is a good idea have a bigger epsilon value so that an agent can take big leaps in learning and explore things. As the agent learns more about future rewards, it is a good idea to decay the epsilon value so that the agent can exploit the informative Q-values found previously. In the current project, we used epsilon-greedy as our action-selection algorithm. We set our initial epsilon value as 1. As the driving agent continued to learn, we decreased the epsilon value exponentially with a decay factor of 0.99 at the end of each episode. By doing this, we could have the agent focus more on exploration in the early stages of learning and gradually switch to a heavier focus on exploitation as he gained more knowledge.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The driving agent would always start an episode from the starting point of the course. Whenever an episode ended, the total reward for the episode was calculated. Whenever he crashed on the side of the road, the current episode would be considered as finished and a new episode would start. Each total episodic reward points started from -10, 000. The further the agent went, the more points the agent would acquire. There were six sequential check points placed on the track. The more check points that were reached, the more points the agent would be rewarded with (i.e., 1000 points for each check point reached). Additionally, an episode would be ended if the learning seemed to be in a dead end. In other words, if the agent spent more than 2000 actions or steps (i.e., going forward, left, or right) in a single episode, we would end the episode and start a new one to save time. It is worth mentioning that the number of steps needed to successfully reach the goal from the starting to the end position should be way less than 2000 steps. Finally, when the agent actually reached the goal, the episode would also end. In this case, the agent would be given a huge reward. It was set that the reward of reaching the goal as 10,000. This reward distinguished the condition from any other unsuccessful attempt.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We built a functionality in the program to allow the user to “skip” the first n episodes. By skip, it means hiding the UI display during the first n episodes to allow speeding up the overall training process (rendering takes time). When the training reached n+1 episode, the visual training animation became observable.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Throughout the entire training, our program was built to plot two learning graphs. The first graph was plotted when the agent beat the game for the first time. At this point, the agent may not have learned how to beat the game entirely. He could have just succeeded by chance and then failed in the coming episodes. The second graph would be plotted when the agent was able to continually beat the game. We consider the second scenario as the training completion condition, which we defined as “the condition of the driving agent successfully self-navigating to the goal of the course for 10 times in a row”.
### ***SARSA***
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To better evaluate the training result of the Q-learning agent, we decided to implement another reinforcement learning algorithm - SARSA as a comparison. Different from Q-learning, SARSA is an on-policy reinforcement learning algorithm that updates the q-table by state and action pair.  A SARSA agent learns q-values based on the action performed by the current policy instead of the greedy policy, which chooses the action that leads to the highest value. Q-learning directly learns the optimal policy, whilst SARSA learns a near-optimal policy as the agent explores. In our case, we implemented the SARSA agent with the same learning rate, decay factor, and exploration rate (i.e., alpha, gamma, and epsilon) used in the Q-learning implementation. By doing this, we could compare the performance of the SARSA agent with the Q-learning agent in a controlled environment. In the plotted Reward-Episode graphs, it was mainly focused on two critical points: the amount of training needed for the agent to reach the finish line for the first time and the amount of training needed for the agent to be able to constantly reach the finish line. By comparing these information side by side between the two algorithm, we intended to evaluate the learning-performance of our Q-learning agent. 
***

## Result and Evaluation
### ***Q-Learning***
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With q-learning, our agent reached the goal for the first time in his 1536th training episode. By that point, the performance was steadily improving with small instability, which we expected to be normal. As long as the learning pattern was shown to be improving overtime, we consider the learning strategy to be effective and correct. At the point that the agent reached the goal for the first time, it started to be able to navigate. However, there was still a possibility that the agent’s behavior and success very still unstable and not maintainable. For this reason, a criterion was specified to define what it means as having learned how to navigate stably and successfully for an agent in the project. As mentioned in the Approach section, we used the condition of the agent consecutively reaching the goal for 10 times in a row as the indication of training completion. It was observed that when he reached the 1098th episode, he fully learned the track and acquired the complete ability of self-navigating to the course goal. It can also be deduced that after the first time the agent beat the game, it took him another 12 episodes to be able to stably beat the game.
### ***Q-Learning vs. SARSA Result Comparison*** 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There is a difference in two reinforcement learning algorithm strategies. The SARSA agent seems to display a less efficient learning pattern comparing to the Q-learning agent.<br/> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The SARSA agent had a less stable increase on the way to the first completion of the driving course. It took him 2796 episodes to self-navigate to the finish line successfully.<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The SARSA agent also took longer to fully learn the self-driving behavior (reaching the goal for 10 times in a row) after the first time reaching the goal. As can be calculated from Figure 4, After the first time the SARSA agent beat the game, he took another 672 episodes to be able to stably beat the game. In other words, only when he reached the 3468th episode, he was then able to fully learn the course and acquired the complete ability of self-navigating to the course goal. In comparison, the Q-learning agent took significantly fewer episodes to reach the same stable state.<br/>

![](https://github.com/JapherS/Self-Navigating-Agent/blob/master/learning_plots/Results.png)
***

## Conclusion and Future Work
### ***Conclusion***
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In conclusion, Q-learning was used to train the self-driving agent and the result was consistent with our expectation. The agent was able to successfully self-navigate through the course after a reasonable amount of training episodes. Apart from that, a SARSA agent was included in the project to compare with the learning result of the q-learning agent. The comparison result showed that the Q-learning agent obtained better learning efficiency and performance than the SARSA agent. 
### ***Limitation***
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are two potential limitations in our current project. It is worth noting that the size and complexity of the track and possible actions of the car in the current project are small and low. In the case where the track size and complexity are significantly greater, there will be limitations on using Q-learning to solve the self-driving agent problem. Because Q-learning requires the usage of a q-table to store learning data, a bigger and more complex track (e.g., more complicated state and higher number of actions) will require a bigger q-table. This may cause a memory issue, which is one potential limitation in the current project. A bigger table also requires more time to be updated, which extends necessary learning time. This causes a time issue and is a second limitation. 
### ***Recommendation***
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To improve the learning efficiency of the agent, we suggest future effort to be focused on adopting a different algorithm that may produce better training efficiency and results. Algorithms such as deep Q-learning may be a good alternative to the self-driving car challenge as it addresses the aforementioned memory-related and time-related concerns. In addition, another suggestion for future direction is to find a better decay strategy for epsilon (and/or alpha) to improve the learning efficiency of the self-driving agent.
***

## References
- Artificial Intelligence A-ZTM: Learn How to Build An AI. (2021). Udemy. https://www.udemy.com/course/artificial-intelligence-az/
- Benefits of Self-Driving Vehicles. (2018, March 19). Coalition for Future Mobility. https://coalitionforfuturemobility.com/benefits-of-self-driving-vehicles/#:%7E:text=Automation%20can%20help%20reduce%20the,risky%20and%20dangerous%20driver%20behaviors.
- Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. ArXiv Preprint ArXiv:1606.01540.
- Matiisen, T. (2015, December 19). Demystifying Deep Reinforcement Learning | Computational Neuroscience Lab. Institute of Computer Science, University of Tartu. https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/
- Pete Shinners (2011). PyGame - Python Game Development. Retrieved from http://www.pygame.org
- Russell, S. J., Norvig, P., & Davis, E. (2010). Artificial intelligence: a modern approach. 3rd ed. Upper Saddle River, NJ: Prentice Hall.
- Salloum, Z. (2019, April 25). Exploration in Reinforcement Learning - Towards Data Science. Medium. https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75
- Wikipedia contributors. (2021, April 5). Q-learning. Wikipedia. https://en.wikipedia.org/wiki/Q-learning#cite_note-auto-1
***

## Video Demo
### ***Training in Progress***
[![Overview](https://img.youtube.com/vi/i6uyzdYZbuo/0.jpg)](https://www.youtube.com/watch?v=i6uyzdYZbuo)
### ***Training Completed***
[![Overview](https://img.youtube.com/vi/zh1jAYCGkEg/0.jpg)](https://www.youtube.com/watch?v=zh1jAYCGkEg)





